{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments for Financial Tweet Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates model training and evaluation for a sentiment analysis model focused on financial tweets. It leverages a BERT-based model to classify tweets as positive, negative, or neutral, providing valuable sentiment insights for financial analysis.\n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "1. **Load and Prepare Data**: Loads the latest data loaders generated by the preprocessing pipeline.\n",
    "2. **Define and Train the Model**: Uses a BERT-based architecture for training.\n",
    "3. **Evaluate and Log Results**: Measures accuracy, processing time, and logs results using MLflow.\n",
    "\n",
    "---\n",
    "\n",
    "**Objective**: To fine-tune a BERT-based model on financial tweet data and log performance metrics to track progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data\n",
    "\n",
    "In this step, we load the latest data loaders generated from the preprocessing pipeline, which contain preprocessed\n",
    "training and validation data for sentiment analysis.\n",
    "\n",
    "The data loaders (`train_loader` and `validation_loader`) contain tokenized tweet text and additional features, like the presence of a source link. These loaders ensure that data is efficiently batched for training and evaluation in the model.\n",
    "\n",
    "Using data loaders allows for streamlined processing and batching, making model training faster and memory-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), \"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News\n"
     ]
    }
   ],
   "source": [
    "print(get_project_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(get_project_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mThe ZenML global configuration version (0.68.1) is higher than the version of ZenML currently being used (0.67.0). Read more about this issue and how to solve it here: \u001b[0m\u001b[1;36m\u001b[0m\u001b[34mhttps://docs.zenml.io/reference/global-settings\u001b[31m#version-mismatch-downgrading\u001b[31m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python3.12/site-packages/zenml/integrations/pytorch/materializers/base_pytorch_materializer.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f)  # nosec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline artifact: e2b98d81-fb4d-45c6-bc00-b422e73bdacc loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pipelines.extract_training_data import extract_latest_loaders\n",
    "\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "dataloaders = extract_latest_loaders()\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Sentiment Analysis Model\n",
    "\n",
    "In this section, we initialize the sentiment analysis model, which is based on a BERT architecture with an additional layer to incorporate the `has_source` feature.\n",
    "\n",
    "**Model Structure**:\n",
    "- The model leverages a pre-trained BERT model as its base.\n",
    "- A fully connected linear layer is added to handle sentiment classification into three classes (positive, neutral, negative).\n",
    "- A dropout layer is used for regularization to help prevent overfitting.\n",
    "\n",
    "**Purpose**: Initializing the model here enables us to fine-tune it specifically for the task of sentiment analysis on financial tweets, leveraging BERT's language understanding along with the custom feature representing source presence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_labels=3):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze the BERT embedding layers to retain base language understanding\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Dropout and two fully connected layers\n",
    "        self.dropout = nn.Dropout(0.4)  # Increased dropout to 0.4\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size + 1, 128)\n",
    "        self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, has_source):\n",
    "        embeddings = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        has_source = has_source.unsqueeze(1)\n",
    "        combined_input = torch.cat((embeddings, has_source), dim=1)\n",
    "        \n",
    "        # Passing through dropout and two FC layers with ReLU\n",
    "        x = self.dropout(combined_input)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and Validation - One Epoch\n",
    "\n",
    "In this section, we define functions to train and validate the model within a single epoch.\n",
    "\n",
    "### Training Workflow\n",
    "- The model is set to training mode, enabling dropout and allowing gradient updates.\n",
    "- For each batch in the training data loader:\n",
    "  - We perform a forward pass to calculate predictions.\n",
    "  - Calculate the loss between predictions and true labels.\n",
    "  - Perform backpropagation to update model parameters based on the calculated gradients.\n",
    "- Training loss is recorded to track the model’s learning progress.\n",
    "\n",
    "### Validation Workflow\n",
    "- The model is switched to evaluation mode, which disables dropout layers for stable predictions.\n",
    "- For each batch in the validation data loader:\n",
    "  - We perform a forward pass to calculate predictions without updating gradients.\n",
    "  - Calculate the loss and accuracy for model performance on unseen data.\n",
    "- Validation loss and accuracy are tracked to assess the model’s generalization capabilities.\n",
    "\n",
    "**Purpose**: Training and validating one epoch at a time allows us to monitor model performance and detect potential issues like overfitting, guiding further training adjustments as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total = 0.\n",
    "\n",
    "    loop = tqdm(\n",
    "        enumerate(dataloader, 1),\n",
    "        total=len(dataloader),\n",
    "        desc=f\"Epoch {epoch}: train\",\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    for _, batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        has_source = batch['has_source'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids = input_ids, attention_mask=attention_mask, has_source=has_source)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix({\"loss\": train_loss/total})\n",
    "\n",
    "    avg_train_loss = train_loss / total\n",
    "    mlflow.log_metric('train_loss', avg_train_loss, step=epoch)\n",
    "\n",
    "\n",
    "def val_one_epoch(model, dataloader, criterion, device, epoch, best_so_far, ckpt_name='model'):\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(\n",
    "            enumerate(dataloader, 1),\n",
    "            total=len(dataloader),\n",
    "            desc=f\"Epoch {epoch}: val\",\n",
    "            leave=True,\n",
    "        )\n",
    "        for i, batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            has_source = batch['has_source'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, has_source=has_source)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n",
    "        current_acc = correct / total\n",
    "\n",
    "        avg_val_loss = val_loss / total\n",
    "        mlflow.log_metric('validation_loss', avg_val_loss, step=epoch)\n",
    "        mlflow.log_metric('validation_accuracy', current_acc, step=epoch)\n",
    "\n",
    "\n",
    "        if current_acc > best_so_far:\n",
    "            print(f\"Validation accuracy improved from {best_so_far:.4f} to {current_acc:.4f}. Saving model...\")\n",
    "            mlflow.pytorch.log_model(model, ckpt_name)\n",
    "\n",
    "            best_so_far = current_acc\n",
    "    return best_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Registration and Updating the Champion Model\n",
    "\n",
    "In this step, we register the trained model in the MLflow Model Registry and update the \"champion\" alias to the latest best-performing version. Registering models and assigning aliases help us manage multiple model versions, allowing for easy deployment of the current best model.\n",
    "\n",
    "### Model Registration\n",
    "- **Purpose**: Registers the trained model under a specified name in the MLflow Model Registry, linking it to a unique run ID.\n",
    "- **Signature**: We define an input and output schema (model signature) to ensure compatibility and verify input format for deployment.\n",
    "- **Result**: This registered model can then be versioned, tracked, and deployed as needed.\n",
    "\n",
    "### Updating the Champion Model\n",
    "- **Champion Alias**: The \"champion\" alias points to the best-performing model version, making it easy to retrieve and use in production.\n",
    "- **Updating the Alias**: After registering the model, we set the \"champion\" alias to the latest model version, which has been validated to perform optimally.\n",
    "\n",
    "By registering the model and updating the champion alias, we maintain organized model versions and ensure easy access to the top-performing model for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def register_model(run_id, model_name, description):\n",
    "    \"\"\"\n",
    "    Registers the incoming model from the specified run without modifying the Champion tag.\n",
    "    \n",
    "    Parameters:\n",
    "    - run_id: str, the ID of the run where the model is logged.\n",
    "    - model_name: str, the name of the model in the registry.\n",
    "    - description: str, a description for the model version.\n",
    "    \n",
    "    Returns:\n",
    "    - version: int, the version of the registered model.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    model_uri = f\"runs:/{run_id}/{model_name}\"\n",
    "    result = mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"Model registered with name '{model_name}' and version '{result.version}'\")\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=result.version,\n",
    "        description=description,\n",
    "    )\n",
    "    return result.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_champion_alias(model_name, metric_name=\"validation_accuracy\"):\n",
    "    \"\"\"\n",
    "    Goes through all model versions, checks their metrics, and assigns the \"Champion\" alias to the best-performing model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_name: str, the name of the model in the MLflow Model Registry.\n",
    "    - metric_name: str, the metric to base the Champion selection on (default is 'accuracy').\n",
    "    \n",
    "    Returns:\n",
    "    - champion_version: int, the version of the model that is now assigned the \"Champion\" alias.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Search for all registered versions of the model\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    \n",
    "    # Initialize variables to track the best version based on the metric\n",
    "    best_version = None\n",
    "    best_metric_value = -float('inf')  # Assume we're maximizing the metric (e.g., accuracy)\n",
    "    best_run_id = None\n",
    "\n",
    "    # Go through all versions to find the one with the best metric\n",
    "    for version in versions:\n",
    "        run_id = version.run_id\n",
    "        # Get the run's metrics\n",
    "        run = client.get_run(run_id)\n",
    "        \n",
    "        if metric_name in run.data.metrics:\n",
    "            metric_value = run.data.metrics[metric_name]\n",
    "            if metric_value > best_metric_value:\n",
    "                best_metric_value = metric_value\n",
    "                best_version = version.version\n",
    "                best_run_id = run_id\n",
    "    \n",
    "    # Check if a best version was found\n",
    "    if best_version is None:\n",
    "        raise ValueError(f\"No models found with metric '{metric_name}'\")\n",
    "\n",
    "    # Reassign the \"champion\" alias to the best version\n",
    "    client.set_registered_model_alias(\n",
    "        name=model_name,\n",
    "        alias=\"champion\",\n",
    "        version=best_version\n",
    "    )\n",
    "    print(f\"Model version {best_version} from run {best_run_id} assigned as 'champion' with {metric_name}: {best_metric_value}\")\n",
    "\n",
    "    return best_version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Main Training Loop\n",
    "\n",
    "In this section, we define the main training loop, where the model is trained and validated across multiple epochs. During each epoch, the model undergoes a cycle of training on the training dataset and then validation on the validation dataset to assess its performance.\n",
    "\n",
    "### Workflow\n",
    "1. **Training Phase**: \n",
    "   - For each epoch, the model performs forward and backward passes on batches from the training set, adjusting weights based on the calculated gradients.\n",
    "   - Training loss is recorded and logged for each epoch to monitor the model’s learning progress.\n",
    "\n",
    "2. **Validation Phase**: \n",
    "   - After each training phase, the model is evaluated on the validation set without updating weights.\n",
    "   - Validation metrics, including loss and accuracy, are tracked to assess the model’s generalization ability on unseen data.\n",
    "\n",
    "3. **Logging with MLflow**:\n",
    "   - For each epoch, training and validation metrics (loss and accuracy) are logged in MLflow, enabling us to track performance over time and compare results across epochs.\n",
    "\n",
    "**Purpose**: The main training loop facilitates iterative learning, helping to evaluate model performance and detect issues like overfitting. Tracking metrics across epochs allows us to determine the point of optimal performance and informs further tuning efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: train: 100%|██████████| 344/344 [02:00<00:00,  2.85it/s, loss=0.768]\n",
      "Epoch 0: val: 100%|██████████| 170/170 [00:20<00:00,  8.26it/s, loss=0.431, acc=0.852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy improved from 0.0000 to 0.8518. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/31 17:45:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Epoch 1: train: 100%|██████████| 344/344 [03:39<00:00,  1.56it/s, loss=0.36] \n",
      "Epoch 1: val: 100%|██████████| 170/170 [00:43<00:00,  3.94it/s, loss=0.329, acc=0.89] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy improved from 0.8518 to 0.8895. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/31 17:50:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Epoch 2: train: 100%|██████████| 344/344 [02:56<00:00,  1.95it/s, loss=0.227]\n",
      "Epoch 2: val: 100%|██████████| 170/170 [00:25<00:00,  6.78it/s, loss=0.307, acc=0.897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy improved from 0.8895 to 0.8973. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/31 17:53:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Epoch 3: train: 100%|██████████| 344/344 [02:30<00:00,  2.28it/s, loss=0.149]\n",
      "Epoch 3: val: 100%|██████████| 170/170 [00:26<00:00,  6.30it/s, loss=0.331, acc=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy improved from 0.8973 to 0.9006. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/31 17:56:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Epoch 4: train:   8%|▊         | 28/344 [00:14<02:43,  1.93it/s, loss=0.102]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>run = mlflow.active_run()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>run_id = run.info.run_id                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> epoch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(epochs):                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>best_so_far = val_one_epoch(model, val_loader, criterion, device, epoch, best_so    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>register_model(run_id, model_name, model_desctiption)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>update_champion_alias(model_name)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_one_epoch</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss = criterion(logits, labels)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>30 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>train_loss += loss.item() * input_ids.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">3.12/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">521</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 520 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 521 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 524 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">3.12/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">289</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat the same comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>289 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_engine_run_backward(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>grad_tensors_,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retain_graph,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">3.12/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">769</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_engine_run_backward</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attach_logging_hooks:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>769 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>t_outputs, *args, **kwargs                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the backward pass</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">772 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m28\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0mrun = mlflow.active_run()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0mrun_id = run.info.run_id                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m epoch \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(epochs):                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m28 \u001b[2m│   │   \u001b[0mtrain_one_epoch(model, train_loader, optimizer, criterion, device, epoch)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0mbest_so_far = val_one_epoch(model, val_loader, criterion, device, epoch, best_so    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   \u001b[0mregister_model(run_id, model_name, model_desctiption)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   \u001b[0mupdate_champion_alias(model_name)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtrain_one_epoch\u001b[0m:\u001b[94m30\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   \u001b[0mloss = criterion(logits, labels)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m30 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   \u001b[0mtrain_loss += loss.item() * input_ids.size(\u001b[94m0\u001b[0m)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m3.12/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m521\u001b[0m in \u001b[92mbackward\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 518 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 519 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 520 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 521 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 522 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 523 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 524 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m3.12/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m289\u001b[0m in \u001b[92mbackward\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat the same comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m289 \u001b[2m│   \u001b[0m_engine_run_backward(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   \u001b[0mtensors,                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   \u001b[0mgrad_tensors_,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   \u001b[0mretain_graph,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/.venv/lib/python\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m3.12/site-packages/torch/autograd/\u001b[0m\u001b[1;33mgraph.py\u001b[0m:\u001b[94m769\u001b[0m in \u001b[92m_engine_run_backward\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m766 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m attach_logging_hooks:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m767 \u001b[0m\u001b[2m│   │   \u001b[0munregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m768 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m769 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m Variable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to \u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   │   \u001b[0mt_outputs, *args, **kwargs                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   │   \u001b[0m)  \u001b[2m# Calls into the C++ engine to run the backward pass\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m772 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "mlflow.set_tracking_uri(f\"file://{project_root}/mlruns\")\n",
    "\n",
    "epochs = 10\n",
    "device = 'mps'\n",
    "model_name = 'simple_sentiment_analysis_model'\n",
    "model_folder = \"simple_sentiment_analysis_model\"\n",
    "lr = 1e-5\n",
    "\n",
    "model_desctiption = \"BERT with two FC layers (128, num_labels), 0.4 dropout, embeddings frozen, with ReLU activation.\"\n",
    "\n",
    "model = SentimentAnalysisModel(bert_model_name='bert-base-uncased', num_labels=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_so_far = 0.\n",
    "mlflow.set_experiment(\"SentimentAnalysis\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    run = mlflow.active_run()\n",
    "    run_id = run.info.run_id\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        best_so_far = val_one_epoch(model, val_loader, criterion, device, epoch, best_so_far, model_name)\n",
    "    register_model(run_id, model_name, model_desctiption)\n",
    "    update_champion_alias(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Saving the Model to the Models Folder\n",
    "\n",
    "After training, we save the model's architecture and weights to the `models` folder. This allows for easy retrieval of the trained model for further experimentation, deployment, or fine-tuning.\n",
    "\n",
    "### Saving Workflow\n",
    "- **Model Architecture**: The architecture of the model, including its layers and configurations, is saved to ensure that the structure can be reconstructed when reloading the model.\n",
    "- **Model Weights**: The learned weights from training are saved, which capture the model's optimized parameters for making predictions.\n",
    "\n",
    "Saving the model in the `models` folder ensures that we have a stored version of the trained model readily available, enabling future use or deployment without re-training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/mlruns'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert_lstm_sentiment_analysis_model loaded successfully\n",
      "Model weights saved to /Users/maxmartyshov/Desktop/IU/year3/PMDL/Sentiment_Analysis_for_Financial_News/models/bert_lstm_sentiment_analysis_model/model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.inference import load_model_from_registry\n",
    "\n",
    "# Load the model from the registry\n",
    "model = load_model_from_registry(model_name, 'champion')\n",
    "\n",
    "# Define the path to save model weights\n",
    "model_folder_path = os.path.join(os.getcwd(), 'models', model_folder)\n",
    "model_weights_pth_path = os.path.join(model_folder_path, 'model_weights.pth')\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "os.makedirs(model_folder_path, exist_ok=True)\n",
    "\n",
    "# Save the model weights to a .pth file\n",
    "torch.save(model.state_dict(), model_weights_pth_path)\n",
    "\n",
    "print(f\"Model weights saved to {model_weights_pth_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
